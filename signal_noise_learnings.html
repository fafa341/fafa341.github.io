<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Signal in the Noise - Learnings | Fabio Fuentes</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
      body {
        background-color: white;
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px;
        color: #1f1f1f;
        max-width: 800px;
        margin: 40px auto;
        padding: 20px;
        line-height: 1.6;
      }
      h1 {
        font-size: 28px;
        margin-bottom: 10px;
      }
      h2 {
        font-size: 20px;
        margin-top: 30px;
        margin-bottom: 10px;
      }
      a {
        color: #1772d0;
        text-decoration: none;
      }
      a:hover {
        color: #f09228;
      }
      .back-link {
        margin-bottom: 30px;
      }
    </style>
  </head>
  <body>
    <div class="back-link">
      <a href="index.html">← Back to main page</a>
    </div>

    <h1>Signal in the Noise: Learnings</h1>
    <p><em>A State-Dependent Local Projection Analysis of News Narratives and the Chilean Business Cycle</em></p>

    <h2>Key Learnings</h2>
    <p>
      Causal inference can be achieved with more specificity. Simpler tools can work better.
    </p>
    <p>
      There's many ways to keep contributing to science with applied ML and NLP. Evaluating different tools and refining them based on research is the way.
    </p>
    <p>
      Essentially predicting individuals is not possible. Other economic agents are different.
    </p>
    <p>
      Maintaining order with folders and files was a must. Documenting every week is needed. Also commenting code is really needed.
    </p>
    <p>
      Learning on demand is what truly drives progress.
    </p>

    <h2>Technical Insights</h2>
    <p>
      LLMs can really leverage your technical work. Usually plan with Gemini Deep research feature and execute with Claude. To avoid hallucinations use Google AI Studio with Gemini as well. Claude was better for understanding HTML and developing scraping, Gemini better at qualitative understanding and performance of tasks.
    </p>

    <h2>Challenges and Solutions</h2>
    <p>
      Scraping old HTML and cleaning data was time consuming and hard. Manipulating LDA to maintain consistency and a solid output was hard. Creating a consistent data pipeline was hard. Developing substantive knowledge about the field over the years was hard (without this, this couldn't have been done).
    </p>

    <h2>Future Work</h2>
    <p>
      To contribute meaningfully to science, I propose evaluating guided, supervised, and unsupervised models alongside LLMs as components in a single modeling stack, rather than in isolation. This allows a clearer analysis of accuracy, time, cost trade-offs under realistic constraints.
    </p>
    <p>
      Here, accuracy is not limited to predictive performance, but includes topic manipulation robustness, semantic consistency over time, and contextual stability.
    </p>
    <p>
      LLMs help bridge gaps in context understanding by allowing machines to model meaning without fully "understanding" it. This enables analysis across three interacting layers:
    </p>
    <ul>
      <li><strong>Representational layer</strong> – statistical associations, embeddings, topic structure</li>
      <li><strong>Latent interpretive layer</strong> – framing, intent, moral stance, power dynamics</li>
      <li><strong>Behavioral consequence layer</strong> – attention allocation, collective coordination, belief shifts</li>
    </ul>
    <p>
      By combining these layers with substantive domain knowledge, it can help uncover causal and interpretive insights in the behavioral sciences, moving beyond surface level fluency toward measurable social impact.
    </p>

  </body>
</html>
